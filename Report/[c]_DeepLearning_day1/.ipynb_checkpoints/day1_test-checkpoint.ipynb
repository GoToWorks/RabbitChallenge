{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出区分【c】深層学習_day1\n",
    "## Section1: 入力層～中間層\n",
    "#### 確認テスト\n",
    "・動物分類の例を図示<br>\n",
    "<img src=\"img/img1.png\" width=\"400\"><br>\n",
    "・pythonで$u=w_1x_1+w_2x_2+w_3x_3+w_4x_4+b$を記述<br>\n",
    "u=np.dot(x,W)+b<br>\n",
    "・中間層の定義を抽出<br>\n",
    "2層の総入力<br>\n",
    "u2 = np.dot(z1, W2) + b2<br>\n",
    "2層の総出力<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3: 出力層\n",
    "#### 確認テスト\n",
    "・線形と非線形の違い<br>\n",
    "<img src=\"img/img2.png\" width=\"400\"><br>\n",
    "・コードの活性化関数に該当する箇所<br>\n",
    "1層の総出力<br>\n",
    "z1 = functions.relu(u1)<br>\n",
    "2層の総出力<br>\n",
    "z2 = functions.relu(u2)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3: 出力層\n",
    "#### 確認テスト\n",
    "●誤差関数に関して<br>\n",
    "・なぜ引き算ではなく2乗するのか<br>\n",
    "=>2乗することで負の値が持つ情報量が保持されるから<br>\n",
    "・2乗平均誤差の1/2はどういう意味か<br>\n",
    "=>微分した際に計算式が簡略化されるから<br>\n",
    "・ソフトマックス関数の処理の説明<br>\n",
    "If文の中はミニバッチとしてデータが取り扱われる時に使用される。<br>\n",
    "x = x - np.max(x)はオーバーフロー対策(expは指数関数的に大きくなるため)<br>\n",
    "np.exp(x) / np.sum(np.exp(x))で確率に落とし込む。全て足すと1になる。\n",
    "\n",
    "    def softmax(x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    " \n",
    "・交差エントロピー処理の説明<br>\n",
    "yは0又は1。ニューラルネットワークがd番目が正解とした場所。<br>\n",
    "対数関数は０に近づくと－∞に発散してしまうため1e-7を足している。<br>\n",
    "  \n",
    "    def cross_entropy_error(d, y):\n",
    "    if y.ndim == 1:\n",
    "        d = d.reshape(1, d.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if d.size == y.size:\n",
    "        d = d.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4： 勾配降下法\n",
    "#### 確認テスト\n",
    "・オンライン学習とは<br>\n",
    "学習データが入ってくるたびに都度パラメータを更新し、学習を進める手法。バッチ学習は一度に全ての学習データを使ってパラメータ更新を行う。<br>\n",
    "・$w^{(t+1)}=w^{(t)}+ε∇E_t$の数式の説明<br>\n",
    "<img src=\"img/img3.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5： 誤差逆伝播\n",
    "#### 確認テスト\n",
    "・誤差逆伝播法で既に行った計算結果を保持しているコードの抽出<br>\n",
    "    delta1 = np.dot(delta2, W2.T) * functions.d_sigmoid(z1)<br>\n",
    "・偏微分の式<br>\n",
    "$\\frac{∂E}{∂y}\\frac{∂y}{∂u}:$delta1 = np.dot(delta2, W2.T) * functions.d_sigmoid(z1)<br>\n",
    "$\\frac{∂E}{∂y}\\frac{∂y}{∂u}\\frac{∂u}{∂w_ji^{(2)}}$=    grad['W1'] = np.dot(x.T, delta1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
