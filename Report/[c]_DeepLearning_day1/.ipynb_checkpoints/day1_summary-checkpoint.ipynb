{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出区分【c】深層学習_day1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1: 入力層～中間層\n",
    "#### 要点まとめ\n",
    "・入力層：$x= \\left(\\begin{array}{c} x_1 \\\\\\ \\vdots \\\\\\ x_l \\end{array}\\right)$ , 重み：$W= \\left(\\begin{array}{c} w_1 \\\\\\ \\vdots \\\\\\ w_l \\end{array}\\right)$の時<br>\n",
    "総入力:$u=w_1x_1+w_2x_2+w_3x_3+\\cdots+w_lx_l+b =Wx+b$<br>\n",
    "$Wx+b$は一次関数のように表すことができ、wは傾きを、bは切片を変えることができる<br>\n",
    "・出力:$z=f(u)$<br>\n",
    "(f:活性化関数)<br>\n",
    "出力zは次の中間層の入力xとして使われる<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2: 活性化関数\n",
    "#### 要点まとめ\n",
    "・活性化関数を使うことで、ニューラルネットワークの幅が広がる<br>\n",
    "・活性化関数には非線形関数を使う<br>\n",
    "・ReLU関数：最も使われている活性化関数。勾配消失問題の回避、スパース化に貢献する。<br>\n",
    "$x= \\left(\\begin{array}{c} x (x >0) \\\\\\ 0 (x <= 0) \\end{array}\\right)$<br>\n",
    "・シグモイド関数：0~1の範囲で緩やかに変化。勾配消失問題がある。<br>\n",
    "・ステップ関数：中間層で用いる。現在は活性化関数として用いられることは少ない。<br>\n",
    "$x= \\left(\\begin{array}{c} 1 (x >=0) \\\\\\ 0 (x < 0) \\end{array}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3: 出力層\n",
    "#### 要点まとめ\n",
    "・出力層は確率を出力する<br>\n",
    "・訓練データ、出力及び誤差関数から導出した誤差が小さくなるようにする<br>\n",
    "・誤差関数は分類問題ではクロスエントロピー誤差を用いる場合が多い<br>\n",
    "・出力層で用いる活性化関数は主に恒等写像、シグモイド関数(二値分類)、ソフトマックス関数（多値分類)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4: 勾配降下法\n",
    "#### 要点まとめ\n",
    "・勾配降下法を利用してパラメータを最適化<br>\n",
    "$w^{(t+1)}=w^{(t)}+ε∇E$<br>\n",
    "$ΔE=\\frac{δE}{δw}=[\\frac{δE}{δw_1}・・・\\frac{δE}{δw_M}]$<br>\n",
    "ε:学習率<br>\n",
    "・εが大きすぎると発散して学習が進まない。εが小さすぎると収束するまでに時間がかかる。<br>\n",
    "・確率的勾配降下法：ランダムに抽出したサンプルの誤差を判定<br>\n",
    "メリット<br>\n",
    " データの冗長な場合の計算コストの軽減<br>\n",
    " 望まない局所極小解に収束するリスクの軽減<br>\n",
    " オンライン学習(バッチ学習と対極)ができる。最初にデータを集める必要がない。リアルタイム。<br>\n",
    "・ミニバッチ勾配降下法：ランダムに分割したデータの集合(ミニバッチ)$D_t$に属するサンプルの平均誤差<br>\n",
    "$w^{(t+1)}=w^{(t)}+ε∇E_t$<br>\n",
    "$E_t=\\frac{1}{N_t}\\sum_{i=n∈D_t}E_n$<br>\n",
    "$N_t=|D_t|$<br>\n",
    "メリット<br>\n",
    " 確率的勾配降下方のメリットを損なわず、計算機の計算資源を有効利用できる<br>\n",
    " →CPUを利用したスレッド並列化やGPUを利用したSIMD(Single Instruction Multi Data)並列化が可能<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5: 誤差逆伝播法\n",
    "#### 要点まとめ\n",
    "・∇Eを計算する際、数値微分を用いると計算量が大きくなるため、誤差逆伝播法を使用する。<br>\n",
    "・出力層からの偏微分を掛けわせて行くことで重みwに対する偏微分を求める。\n",
    "$\\frac{∂E}{∂w^{(2)}}=\\frac{∂E}{∂y}\\frac{∂y}{∂u}\\frac{∂u}{∂w_ji^{(2)}}=(y-d)*\\left(\\begin{array}{c} 0\\\\\\ \\vdots \\\\\\ z_j\\\\\\ \\vdots \\\\\\ 0 \\end{array}\\right)=(y_j-d_j)z_j$<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
