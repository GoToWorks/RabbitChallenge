{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出区分【b】機械学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形回帰モデル\n",
    "### 要点のまとめ\n",
    "・線形とは比例のことy=ax+b。直線で予測<br>\n",
    "・n次元空間における超平面では<br>\n",
    "$y=a_0+a_1x_1+a_2x_2+・・・+a_{n-1}x_{n-1}$<br>\n",
    "$=\\sum_{i=0}^{n-1}a_ix_i (x_0=1)$<br>\n",
    "$=a^Tx(ベクトル表記)$<br>\n",
    "・予測値にはハットをつける$\\hat{y}$<br>\n",
    "・一次元としていてx_1以外の要素があった場合全て誤差εに吸収されてしまう<br>\n",
    "・連立方程式⇔行列式の変換は常に意識する<br>\n",
    "・全データは学習用データと検証用データに分けて汎化性能を検証する<br>\n",
    "・パラメータは最小二乗法で推定する<br>\n",
    "・二乗損失は一般に外れ値に弱い<br>\n",
    "・最尤法による予測値<br>\n",
    "$\\hat{y}=X_*(X^TX)^-1X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非線形回帰モデル\n",
    "### 要点のまとめ\n",
    "・非線形は曲線で予測<br>\n",
    "e.g.,$y=w_0+w_1x+w_2x^2+w_3x^3$,$y=w_0+w_1sinx+w_2cosx+w_3logx$<br>\n",
    "・線形との違いはxがφ(x)に代わるだけ<br>\n",
    "・xからφ(x)に代わってもwについて線形(linear-in-parameter)<br>\n",
    "・Gauss型基底関数<br>\n",
    "$φ_i(x)=exp(-(x-μ_i)^2/2h_i)$<br>\n",
    "・最尤法による予測値<br>\n",
    "$\\hat{y}=φ_*(φ^Tφ)^-1φ^Ty$<br>\n",
    "・未学習(underfitting)->学習データに対して十分小さな誤差が得られないモデル。次元をあげる必要あり<br>\n",
    "・過学習(overfitting)->学習データに対しての誤差は小さいが、新たなデータに対しての誤差が大きい<br>\n",
    " -対策1.学習データを増やす<br>\n",
    " -対策2.不要な基底関数を削除する<br>\n",
    " -対策3.正則化法を利用して表現力を抑止する<br>\n",
    "・学習データと検証データの分け方<br>\n",
    " -ホールドアウト法:最初に学習と検証データを分ける。1度しか検証できない<br>\n",
    " -クロスバリデーション法：データを分割し、分割したデータのうち1つ検証データにする。これを分割した分だけ繰り返す<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰モデル\n",
    "### 要点のまとめ\n",
    "・分類。目的変数が0or1<br>\n",
    "・識別的アプローチ:$P(C_k|x)$を直接model化する。識別関数の構成もある(f(x)>0ならC=1,F(x)<0ならC=0)<br>\n",
    "・生成的アプローチ:$P(C_k)$と$P(x|C_k)$をmodel化し、その後ベイズの定理を用いて$P(C_k|x)$を求める<br>\n",
    "・線形結合にシグモイド関数σを用いることで0~1の間の確率に変換する<br>\n",
    "$P(Y=1|x)=σ(w_0+w_1x^1+...+w_mx^m)$<br>\n",
    "・尤度関数：データは固定し、パラメータを変化させる。尤度関数を最大化するようなパラメータの推定方法を最尤推定という<br>\n",
    "$\\prod p^{y_i}(1-p)^{1-y_i}$<br>\n",
    "・尤度関数と対数尤度関数が最大になる点は同じ。桁落ちしないように対数をとる<br>\n",
    "・勾配降下法で重みwを反復的に更新していく<br>\n",
    "$w^{(k+1)}=w^{(k)}+η\\sum_{i=1}^{n}(y_i-p^i)x_i$<br>\n",
    "・確率的勾配降下法:上記の式でnが大きいと一回の更新でメモリを大量に消費するので、更新に使うiを限定して素早く更新する<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析\n",
    "### 要点のまとめ\n",
    "・分散が最大となる(情報が残っている)ように次元削減を行う<br>\n",
    "・線形変換後の変数の分散が最大となる射影軸を探索する<br>\n",
    "$Var(s_j)=1/ns_j^Ts_j=1/n(Xa_j)^T(Xa_j)=1/na_j^TX^TXa_j=a_j^TVar(X)a_j$<br>\n",
    "・目的関数arg max $a_j^TVar(X)a_j$を解けば良いが、無限に解が出てくるので制約をつける<br>\n",
    "・制約付き最適化問題を解くときは、ラグランジュ関数を最大にする係数ベクトルを探索する(微分して0になる点)<br>\n",
    "$E(a_j)=a_j^TVar(X)a_j-λ(a_j^Ta_j-1)$<br>\n",
    "(制約条件：ノルムが1。$a_j^Ta_j=1$)<br>\n",
    "・上記を$a_j$に対して偏微分すると$Var(X)a_j=λa_j$となり、射影先の分散は固有値と一致する<br>\n",
    "・寄与率を用いて、次元削減後の情報保持率を確認する<br>\n",
    "$r_k=\\frac{\\sum_{j=1}^kλ_j(第1~k主成分の分散)}{\\sum_{i=1}^mλ_i(主成分の総分散)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アルゴリズム\n",
    "### 要点のまとめ\n",
    "k近傍法<br>\n",
    "・最近傍のデータをk個取ってきて、それらがもっとも多く所属するクラスに識別する。<br>\n",
    "・kを変化させると結果も変わる。kはアルゴリズムを回す前に決めるパラメータ<br>\n",
    "k-means<br>\n",
    "・教師なし学習、クラスタリング手法<br>\n",
    "・与えられたデータをk個のクラスタに分割する<br>\n",
    "・k個の中心と各データとの距離を測定し、一番近い中心のクラスタに入れて中心を更新する<br>\n",
    "・中心の初期値が近いとうまくクラスタリングができない可能性がある<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サポートベクターマシーン\n",
    "### 要点のまとめ\n",
    "・2クラス分類を行う<br>\n",
    "・決定関数$f(x)=w^Tx+b$はベクトルxの入力に対してスカラーの値を返す関数<br>\n",
    "・ラベルy、sgn(・)を符号関数(引数が正なら+1,負なら-1)は$y=sgn f(x)$と表せる(f(x)の正負で特徴ベクトルxのクラスを識別)<br>\n",
    "・マージン：分離境界を挟んで2つのクラスがどれくらい離れているか<br>\n",
    "・マージン最大化：複数のマージンの取り方があるが、SVMの視点ではマージンが大きいほど良い分類境界となる<br>\n",
    "・ハードマージン：分離可能性を仮定したSV分類のこと<br>\n",
    "・分類境界に最も近いデータ$x_i$のみが分類境界の決定に寄与する。この$x_i$のことをサポートベクトルと呼ぶ<br>\n",
    "・ソフトマージン：訓練データを完璧に分類できる決定関数f(x)が存在しない場合。スラック関数を追加し、多少の誤差$ξ$を許容する<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
