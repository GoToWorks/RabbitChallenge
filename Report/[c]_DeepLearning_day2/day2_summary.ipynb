{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出区分【c】深層学習_day2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1: 勾配消失問題\n",
    "#### 要点まとめ\n",
    "・勾配消失問題:誤差逆伝播法が下位層に進んでいくについれて、勾配がどんどん緩やかになること。下位層のパラメータはほとんど変わらず、訓練は最適値に収束しなくなる<br>\n",
    "・勾配消失を防ぐ方法は、活性化関数を変更する、初期値を検討する、バッチ正規化を行うの3つがある<br>\n",
    "・活性化関数の検討<br>\n",
    "ReLU関数:微分後は1か０になるため、勾配の消失を回避できる。<br>\n",
    "・重みの初期化設定<br>\n",
    "正規分布に従った乱数で重みを設定するのがXavierの初期値。Xavierはシグモイド関数等のS字関数に効果を発揮する。ReLU関数にはHeの初期化。<br>\n",
    "・バッチ正規化<br>\n",
    "ミニバッチ単位で、入力値のデータの偏りを抑制する手法。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2: 学習率最適化手法\n",
    "#### 要点まとめ\n",
    "・学習率の最適化手法は主にモメンタム、AdaGrad、RMSProp、adamがある。<br>\n",
    "・モメンタム<br>\n",
    "$V_t=μV_{t-1}-ε∇E$<br>\n",
    "$w^{(t+1)}=w^{(t)}+V_t$<br>\n",
    "慣性:μ<br>\n",
    "・AdaGrad：誤差をパラメータで微分したものと再定義した学習率の積を減算<br>\n",
    "$h_0=θ$<br>\n",
    "$h_t=h_{t-1}+(∇E)^2$<br>\n",
    "$w^{(t+1)}=w^{(t)}-ε\\frac{1}{√h_t+θ}∇E$<br>\n",
    "・RMSProp:AdaGradを改良。大域最適解になりやすい<br>\n",
    "$h_t=αh_{t-1}+(1-α)(∇E)^2$<br>\n",
    "$w^{(t+1)}=w^{(t)}-ε\\frac{1}{√h_t+θ}∇E$<br>\n",
    "・adam:モメンタムの過去の勾配の指数関数的減衰平均とRMSPropの過去の勾配の2乗の指数関数的減衰平均のメリットを孕んだアルゴリズム<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3: 過学習\n",
    "#### 要点まとめ\n",
    "・過学習:テスト誤差と訓練誤差とで学習曲線が乖離すること<br>\n",
    "・過学習が起こる原因:パラメータ数が多い、パラメータの値が適切でない、ノードが多い等<br>\n",
    "・正則化:ネットワークの自由度を制約すること。これにより過学習を抑制する<br>\n",
    "L1正則化(ラッソ回帰)、L2正則化(リッジ回帰)。誤差関数に正則化項を加える事で学習用データに対する極小値をずらす<br>\n",
    "$E_n(w)+\\frac{1}{p}λ||x||_p$<br>\n",
    "$||x||_p=(|x_1|^p+...+|x_n|^p)^\\frac{1}{p}$<br>\n",
    "p=1の場合、L1正則化<br>\n",
    "P=2の場合、L2正則化<br>\n",
    "・ドロップアウト<br>\n",
    "中間層のノードをランダムに削除して学習させること。データのバリエーションが生まれ過学習を抑止できる。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4: 畳み込みニューラルネットワークの概念\n",
    "#### 要点まとめ\n",
    "・畳み込みニューラルネットワーク(CNN):次元的な繋がりがあるデータに使える。今までの全結合層のニューラルネットワークだと縦横チャネルの3次元の情報を扱うことができない<br>\n",
    "<img src=\"img/img2.png\" width=\"400\"><br>\n",
    "・畳み込み層:入力値*フィルター =>出力値+バイアス=>活性化関数f(x)=>出力値<br>\n",
    "・全結合層:データを人間が欲しい結果に変換する<br>\n",
    "・パディング:畳み込みを行うと出力層のサイズが小さくなってしまうので、入力層の外側の周りを0等で埋める事で出力データのサイズを調整する<br>\n",
    "・ストライド:フィルターを動かす大きさ<br>\n",
    "・チャンネル:フィルターの数<br>\n",
    "・プーリング層：畳み込みのようにフィルターは用いず、入力値をスライドさせながらMax値や平均値を出力値とする。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5: 最新のＣＮＮ\n",
    "#### 要点まとめ\n",
    "・AlexNet<br>\n",
    "<img src=\"img/img3.png\" width=\"400\"><br>\n",
    "・全結合層への変換<br>\n",
    "Fratten:全て一列に並べる<br>\n",
    "Global Max Pooling:各チャネル毎に1地番大きなものを用いる<br>\n",
    "Global Avg Pooling:各チャネル毎に平均を用いる<br>\n",
    "Frattenの方が情報量が多く良く見えるが、Global Max Pooling、Global Avg Poolingの方が経験上良い結果になる<br>\n",
    "・過学習を防ぐ施策であるドロップアウトはCNNでは全結合層で用いられることが多い<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
