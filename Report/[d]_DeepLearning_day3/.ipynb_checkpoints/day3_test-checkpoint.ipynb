{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出区分【d】深層学習_day3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1: 再起型ニューラルネットワークの概念\n",
    "#### 確認テスト\n",
    "・サイズ5x5の入力画像を、サイズ3x3のフィルタで畳み込んだ時の出力画像のサイズを答えよ。なお、ストライドは２、パディングは1とする<br>\n",
    "出力画像のサイズは3x3<br>\n",
    "・RNNのネットワークには大きく分けて3つの重みがある。1つは入力から現在の中間層を定義する際にかけられる重み、1つは中間層から出力を定義する際にかけられる重みである。残り1つの重みについて説明せよ。<br>\n",
    "前の中間層から現在の中間層への入力にかけられる重み<br>\n",
    "・連鎖律の原理を使い、$dz/dx$を求めよ。$z=t^2, t=x+y$<br>\n",
    "$\\frac{dz}{dx}=\\frac{dz}{dt}\\frac{dt}{dx}=(2t)*1=2t=2(x+y)$<br>\n",
    "・下の図の$y_1$を$x,z_0,z_1,w_{in},w,w_{out}$を用いて数式で表せ。また、中間層の出力にシグモイド関数g(x)を作用せよ。<br>\n",
    "<img src=\"img/img1.png\" width=\"600\"><br>\n",
    "$y_1=g(W_{(out)}・z_1+c)$<br>$z_1=W_{(in)}・x_1+W・z_0+b$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2: LSTM\n",
    "#### 確認テスト\n",
    "・シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものは何か<br>\n",
    "0.25<br>\n",
    "・LSTMで単語の予測をする際、なくなっても影響を及ばさない単語がある場合に作用するゲートは何か<br>\n",
    "忘却ゲート<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3: GRU\n",
    "#### 確認テスト\n",
    "・LSTMとCECが抱える課題について簡潔に述べよ<br>\n",
    "LSTMの問題:パラメータ数が多いため計算負荷がかかる<br>\n",
    "CECの問題:学習機能がない<br>\n",
    "・LSTMとGRUの違い<br>\n",
    "LSTMのパラメータ数を減らし、計算負荷を下げたものがGRU<br>\n",
    "LSTMにはCEC、入力ゲート、出力ゲートを使用するが、GRUはリセットゲートと更新ゲートを使用する<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4: 双方向RNN\n",
    "#### 確認テスト\n",
    "なし"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5: Seq2Seq\n",
    "#### 確認テスト\n",
    "・seq2seqについて説明しているものを選べ<br>\n",
    "双方向RNN:(1)時間に関して順方向と逆方向のRNNを構成し、その2つの中間層表現を特徴料とする<br>\n",
    "seq2seq:(2)RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる<br>\n",
    "構文解析器:(3)構文木などの木構造に対して、隣接単語から表現ベクトルを作る<br>\n",
    "LSTM:(4)RNNの一種であり、勾配消失問題をCECとゲートの概念を導入することで解決する<br>\n",
    "・VAEに関する説明<br>\n",
    "自己符号化器(オートエンコーダ)の洗剤変数に確率分布を導入したもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section6: Word2vec\n",
    "#### 確認テスト\n",
    "・なし"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section7: Attention Mechanism\n",
    "#### 確認テスト\n",
    "・RNNとWord2vec,Seq2Seq,Attentionの違い<br>\n",
    "RNN:時系列データを処理するのに適したネットワーク<br>\n",
    "Word2vec:単語をベクトル表現する手法。単語をone-hotベクトルに変換し、Embeddingに変換する<br>\n",
    "Seq2Seq:1つの時系列データから別の時系列データを得る<br>\n",
    "Attention:時系列データに対して関連性の重みをつける<br>\n",
    "・Seq2SeqとHRED、HREDとVHREDの違い<br>\n",
    "Seq2Seqは1つの時系列データから別の時系列データを得る。HREDはSeq2Seqに文脈の解釈を付け加えたもの<br>\n",
    "VHREDは文章の多様性を求めるためHREDを改良したもの<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
