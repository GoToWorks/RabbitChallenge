{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出区分【d】深層学習_day3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1: 再起型ニューラルネットワークの概念\n",
    "#### 要点まとめ\n",
    "・RNN：時系列データ(時間的に繋がりがあるデータ。例：音声データ、テキストデータ)に対応可能な再帰的なニューラルネットワーク<br>\n",
    "・中間層からの出力を出力層に出すだけでなく、再度中間層の入力に使用する<br>\n",
    "・重みは3種類あり、入力から中間層、中間層から出力、前の中間層からの入力にかけられる<br>\n",
    "<img src=\"img/img1.png\" width=\"600\"><br>\n",
    "・RNNの逆伝播はBPTT(Back Propagation Through Time)を使用する<br>\n",
    "$\\frac{∂E}{∂W_{(in)}}=\\frac{∂E}{∂u^t}\\left(\\begin{array}{c} \\frac{∂u^t}{∂W_{(in)}} \\end{array}\\right)^T=δ^t[x^t]^T$<br>\n",
    "$\\frac{∂E}{∂W_{(out)}}=\\frac{∂E}{∂v^t}\\left(\\begin{array}{c} \\frac{∂v^t}{∂W_{(out)}} \\end{array}\\right)^T=δ^{out,t}[z^t]^T$<br>\n",
    "$\\frac{∂E}{∂W}=\\frac{∂E}{∂u^t}\\left(\\begin{array}{c} \\frac{∂u^t}{∂W} \\end{array}\\right)^T=δ^t[z^{t-1}]^T$<br>\n",
    "$\\frac{∂E}{∂b}=\\frac{∂E}{∂u^t}\\frac{∂u^t}{∂b}=δ^t$<br>\n",
    "$\\frac{∂E}{∂c}=\\frac{∂E}{∂v^t}\\frac{∂v^t}{∂c}=δ^{out,t}$<br>\n",
    "・パラメータ更新。前の中間層の情報を引き継ぐところではΣが出てきている。εは学習率(どれくらい学習に使うかの比率)<br>\n",
    "<img src=\"img/img2.png\" width=\"300\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2: LSTM\n",
    "#### 要点まとめ\n",
    "・RNNの課題：時系列を遡れば遡るほど、勾配が消失していく。長い時系列の学習が困難<br>\n",
    "・勾配消失の解決方法とは、別で構造自体を変えて解決したのがLSTM<br>\n",
    "・勾配爆発：層を逆伝播する毎に指数関数的に勾配が大きくなっていくこと<br>\n",
    "・LSTMの構造<br>\n",
    "<img src=\"img/img3.png\" width=\"600\"><br>\n",
    "・CEC：記憶機能。学習機能はなし。時間依存度に関係なく、重みが一律<br>\n",
    "・入力ゲート:学習機能あり。今回の入力値と前回の中間層の入力値をCECに情報を覚えさせる<br>\n",
    "・出力ゲート:学習機能あり。CECが覚えている情報をどの程度利用するかを学習する<br>\n",
    "・忘却ゲート:学習機能あり。CECは過去の情報も記憶し利用するため、いらない情報を消去する<nr>\n",
    "・覗き穴結合:入力ゲート、出力ゲートで学習する情報にCECの情報を利用する<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3: GRU\n",
    "#### 要点まとめ\n",
    "・LSTMではパラメータが多く、計算負荷がかかる問題がある<br>\n",
    "・GRUはLSTMのパラメータ削減して計算負荷を低くする<br>\n",
    "・CECや入力ゲート、出力ゲートをなくし、リセットゲートと更新ゲートを利用<br>\n",
    "<img src=\"img/img4.png\" width=\"600\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4: 双方向RNN\n",
    "#### 要点まとめ\n",
    "・双方向RNN:過去の情報だけでなく、未来の情報を髪することで精度を向上させるモデル。文章の推敲や、機械翻訳等<br>\n",
    "<img src=\"img/img5.png\" width=\"400\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5: Seq2Seq\n",
    "#### 要点まとめ\n",
    "・Encoder-Decoderモデルの一種。機械対話や、機械本屋になどに使用される<br>\n",
    "・EncoderRNN:ユーザーがインプットしたテキストデータを単語等のトークに区切って渡す構造<br>\n",
    "単語1つ1つに番号を振ることでone-hotベクトルで表現。数万程度のone-hotベクトルをembeddingで数百程度のベクトルへ変換する(同じような単語は同じようなベクトルにする)<br>\n",
    "・MLM(Masked Language Model):一部分をマスクして前後の文脈で何が入るかを予測させながら学習する。ランダムに単語を落とすことで学習できるのでラベル化がいらない。教師なし学習<br>\n",
    "・DecoderRNN:Encoderが文脈を解釈し、Decoderがそれを利用する。システムがアウトプットデータを単語等のトークン毎に生成する構造<br>\n",
    "・Seq2Seqの課題:一問一答のみ。問に対して文脈も何もなく、ただ応答が行われ続ける。=>文脈を考慮する場合はHREDを使用する<br>\n",
    "・HRED:過去のn-1個の発話から次の発話を生成する。HRED=Seq2Seq + Context RNN<br>\n",
    "・Context RNN:Encoderのまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する構造で過去の発話を加味できる<br>\n",
    "・HREDの課題:ありがちな回答になる。文章の多様性がなくなる=>多様性を求めるためVHREDを使用する<br>\n",
    "・オートエンコーダ:教師なし学習の一つ。Encoder,Decoderからなる構造。入力をEncoderで潜在変数zにし、zをDecoderで出力にする。入力と出力のサイズが同じになるようにする<br>\n",
    "・VAE:・オートエンコーダ改良。データを潜在変数zの確率分布に落とし込める<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section6: Word2vec\n",
    "#### 要点まとめ\n",
    "・Word2vec:単語をベクトル表現する手法<br>\n",
    "・単語をone-hotベクトルに変換し、Embeddingに変換すること<br>\n",
    "・大規模データの分散表現の学習が、現実的な計算速度とメモリ量で実現可能になった<br>\n",
    "・Word2vecで処理したものをSeq2Seqなどに利用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section7: Attention Mechanism\n",
    "#### 要点まとめ\n",
    "・Seq2Seqは長い文章への対応が難しい<br>\n",
    "・Attention Mechanism:文章の中で重要なものを拾い上げることができる<br>\n",
    "・近年の自然言語処理はほぼ全てがAttention<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
